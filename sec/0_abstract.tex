\begin{abstract}
This work presents \textbf{\textit{LLM-RL-Tuner}}, \blue{a novel and holistic framework for the end-to-end automation of Reinforcement Learning (RL) agent development, driven by Large Language Models (LLMs).} \blue{RL training is plagued by a fragmentation of expertise, requiring manual selection of algorithms, painstaking design of reward functions, and extensive hyperparameter tuning. Existing automated solutions, such as \textbf{EUREKA} and \textbf{AgentHPO}, typically address these challenges in isolation.} \blue{While end-to-end frameworks like \textbf{Agent$^2$} demonstrate the potential for complete automation, they remain closed-source and inaccessible to the broader research community.} \blue{Our \textit{LLM-RL-Tuner} integrates these critical stages into a single, cohesive workflow: it autonomously performs \textbf{Algorithm Selection} based on the environment's action space, conducts \textbf{Reward Function Optimization} (drawing inspiration from EUREKA's code-writing capabilities), and iteratively executes \textbf{Hyperparameter Tuning} (leveraging AgentHPO's log-driven reasoning).} The LLM serves as an intelligent agent, \blue{utilizing semantic reasoning to analyze the environment's code and training metrics (e.g., loss curves, convergence rate) to propose executable code and configuration changes in a closed-loop fashion.} \blue{By unifying these components on a standard \textbf{Gymnasium} platform and making the framework open-source, \textit{LLM-RL-Tuner} democratizes access to end-to-end AutoRL capabilities, providing a lightweight, accessible alternative to complex, proprietary systems.} Experimental results \blue{on our Gymnasium-compatible environments} show that \blue{this integrated LLM-guided approach significantly enhances training efficiency, stability, and final performance across diverse RL tasks compared to manual or default baselines.}
\end{abstract}

\section{Introduction}
\label{sec:intro}

\blue{Reinforcement Learning (RL) has emerged as a powerful paradigm for sequential decision-making and control, achieving remarkable success in domains ranging from robotics manipulation to strategic game playing. However, the practical deployment of RL agents remains severely constrained by a fundamental challenge: the fragmentation of expertise required throughout the development lifecycle. Unlike supervised learning, where the optimization objective is typically fixed and differentiable, RL involves optimizing policies based on non-stationary, often non-differentiable signals derived from environment interactions. This inherent complexity creates a tri-lemma that constitutes the primary bottleneck in RL development.}

\blue{The first dimension of this tri-lemma is Algorithm Selection. The choice of learning algorithm fundamentally determines both sample efficiency and training stability. On-policy methods such as Proximal Policy Optimization (PPO) offer stability but require extensive interaction with the environment, while off-policy methods like Soft Actor-Critic (SAC) or Deep Q-Networks (DQN) provide superior sample efficiency at the cost of increased complexity and potential instability. This selection decision is rarely automated and primarily relies on heuristic knowledge about action space characteristics (discrete versus continuous) and observation modalities (image versus vector). An incorrect algorithm choice can lead to complete training failure, regardless of how well-crafted subsequent optimization stages may be.}

\blue{The second dimension is Reward Engineering. The reward function serves as the compass guiding agent behavior, and designing dense, shaped reward signals that encourage desired behaviors without inducing reward-hacking behaviors represents one of the most manual and error-prone stages in RL development. Sparse rewards often lead to learning stagnation, while excessive reward shaping can introduce bias that prevents the agent from discovering optimal policies. This challenge is particularly acute in complex environments where the relationship between actions and outcomes is non-obvious.}

\blue{The third dimension is Hyperparameter Optimization. RL algorithms exhibit extreme sensitivity to parameters such as learning rates, entropy coefficients, and buffer sizes. A configuration that converges successfully under one random seed may diverge completely under another, making the debugging process both time-consuming and frustrating. Unlike supervised learning, where hyperparameter tuning can leverage validation sets, RL requires expensive environment interactions to evaluate each configuration, creating a fundamental trade-off between exploration breadth and computational cost.}

Traditional approaches to addressing these challenges rely on statistical search methods such as grid search, random search, and Bayesian optimization. While these methods have demonstrated effectiveness in local tuning scenarios, they suffer from fundamental limitations. They treat the RL agent and environment as black boxes, optimizing scalar parameters based solely on scalar reward feedback without understanding the underlying causal relationships. For instance, a Bayesian optimizer does not recognize that sparse reward signals may require curriculum learning strategies, or that continuous action spaces preclude the use of discrete-action algorithms like DQN. This lack of semantic understanding leads to extremely low sample efficiency, as the optimizer must fail thousands of times to map a landscape that a human expert or language model could infer by reading the environment source code.

\blue{Large Language Models (LLMs), trained on vast repositories of code, technical documentation, and research papers, possess a unique capacity for white-box reasoning. Unlike traditional statistical methods, LLMs can interpret the semantics of task code, understanding that a robotic arm manipulation task implies continuous control requirements, or that pixel-based observations necessitate convolutional neural network architectures. This semantic reasoning capability introduces a paradigm shift from numerical black-box optimization to code-aware, reasoning-based automation.}

\subsection{Existing Solutions and Literature}
\label{subsec:literature}

The current landscape of Automated RL (AutoRL) involves several disjointed approaches, each addressing isolated aspects of the tri-lemma. Statistical AutoRL methods, exemplified by Bayesian Optimization, are effective for local hyperparameter tuning but fundamentally lack causal reasoning capabilities. They rely on extensive computational sampling to map the optimization landscape, which becomes prohibitively inefficient for complex RL tasks where each training run is computationally costly~\cite{cite:agent_hpo}.

\blue{\textbf{EUREKA}~\cite{cite:eureka} represents a breakthrough in automated reward design. It leverages Large Language Models to accept environment source code and natural language task descriptions as context, autonomously writing and refining executable reward functions through an evolutionary search process. The key innovation lies in reward reflection, where the system analyzes detailed scalar breakdowns of reward components from training statistics and uses this feedback to iteratively improve the reward code. EUREKA has demonstrated human-level or superhuman performance in reward engineering across diverse robotics tasks. However, EUREKA operates under a critical assumption: the learning algorithm and hyperparameters are fixed. It focuses exclusively on reward engineering, leaving the practitioner to manually select the appropriate algorithm and tune hyperparameters through separate processes.}

\blue{\textbf{AgentHPO}~\cite{cite:agent_hpo} addresses the hyperparameter optimization dimension through an intelligent agent framework. It functions as a diagnostic analyst, analyzing detailed training logs and historical trial feedback to semantically reason about model performance. When the system detects pathological patterns such as overfitting (indicated by diverging training and validation losses) or instability (manifested as oscillating loss curves), it provides textual diagnoses rather than mere numerical feedback. The framework then proposes hyperparameter adjustments based on logical deduction rather than random search. However, AgentHPO assumes that both the reward function and model architecture are already optimal, operating under the premise that only hyperparameters require refinement.}

\blue{Concepts such as \textbf{Agent$^2$}~\cite{cite:agent2} propose generator-target architectures for full end-to-end automation, where a generator agent analyzes tasks and produces target agents. However, such systems are often closed-source or architecturally complex, creating significant barriers for general research adoption and reproducibility. Furthermore, they may require specialized infrastructure or proprietary components that limit accessibility for the broader research community.}

While methods like Population-Based Training have improved automation through evolutionary strategies, they largely depend on statistical heuristics rather than conceptual reasoning about the environment structure. \blue{This landscape reveals a critical gap: there exists no lightweight, open-source-friendly framework that integrates algorithm selection, reward design, and hyperparameter tuning into a single coherent optimization loop, specifically tailored for standard Gymnasium environments that are accessible to researchers without specialized hardware infrastructure.}

\subsection{Our Solution: LLM-RL-Tuner}
\label{subsec:solution}

\blue{The motivation for this work stems from recognizing that the fragmentation of expertise in RL development creates a fundamental inefficiency. When EUREKA designs an excellent reward function but the wrong algorithm is selected, the reward design effort is wasted. When AgentHPO optimizes hyperparameters for a poorly shaped reward function, convergence remains elusive. The interdependencies between these three stages suggest that treating them in isolation is suboptimal. We hypothesize that a unified framework that allows the LLM to reason across all three dimensions simultaneously will achieve superior performance through synergistic optimization.}

\blue{To address this gap, we propose \textbf{LLM-RL-Tuner}, a holistic framework that integrates a Large Language Model (Claude 4.5) into the entire RL training lifecycle. Unlike prior work that isolates reward design from parameter tuning, our framework implements a three-stage Interleaved Optimization Loop that addresses the complete tri-lemma:}

\begin{enumerate}
    \item \blue{\textbf{Semantic Algorithm Selection:} The LLM analyzes the environment source code, specifically examining action space and observation space specifications, to autonomously select the most appropriate RL algorithm. For discrete action spaces, it may recommend DQN or PPO based on task complexity. For continuous action spaces, it selects from SAC, TD3, or PPO based on sample efficiency requirements. This stage addresses the cold start problem in AutoRL, where poor initial algorithm choices prevent any meaningful learning signal from emerging.}
    
    \item \blue{\textbf{Evolutionary Reward Engineering:} Drawing inspiration from EUREKA, the system generates executable reward functions through iterative code generation. The LLM receives environment state variables and task descriptions, then produces Python-formatted reward expressions that emphasize desired behaviors. Unlike EUREKA, which relies on massive parallel simulation in Isaac Gym, our framework utilizes short-horizon probe training to validate learning signals efficiently, making it feasible for standard Gymnasium environments running on CPU-based systems.}
    
    \item \blue{\textbf{Log-Driven Hyperparameter Refinement:} Leveraging the diagnostic reasoning logic of AgentHPO, the LLM analyzes textual summaries of training logs, including loss curves, entropy variance, and gradient norms, to fine-tune hyperparameters. When the system detects signs of instability or slow convergence, it proposes targeted adjustments such as learning rate decay or entropy coefficient modification, ensuring stability and convergence.}
\end{enumerate}

\blue{LLM-RL-Tuner distinguishes itself by explicitly providing both the environment source code and training dynamics to the LLM, enabling semantic reasoning rather than blind numerical search. The framework operates as a closed-loop system where each stage informs the others: algorithm selection influences reward design requirements, reward quality affects hyperparameter sensitivity, and hyperparameter stability enables more accurate reward evaluation. The objective of this study is to demonstrate that this integrated, reasoning-based approach achieves faster convergence, higher stability, and superior final performance compared to fixed-baseline configurations and isolated optimization methods within Gymnasium-compatible environments such as Arkanoid, Pingpong, and Proly.}
